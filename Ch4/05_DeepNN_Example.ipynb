{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "05_DeepNN_Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLNg_Puse6EX"
      },
      "source": [
        "In this notebook we will demonstrate different text classification models trained using the IMDB reviews dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4DNH1aoeEhE",
        "outputId": "5304f679-8132-4cc3-c50a-c73502c7d44c"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=ff413eaabaf2d63367837f0f054d0f5b2518684987287143f3db7927f5fdd75e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqUcb7NBb5--"
      },
      "source": [
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MqW5vWwfiCP"
      },
      "source": [
        "Here we set all the paths of all the external datasets and models such as [glove](https://nlp.stanford.edu/projects/glove/) and [IMDB reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUKTqLHud7fo",
        "scrolled": false
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    \n",
        "    from google.colab import files\n",
        "    \n",
        "    !wget -P DATAPATH http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip DATAPATH/glove.6B.zip -d DATAPATH/glove.6B\n",
        "    \n",
        "    !wget -P DATAPATH http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xvf DATAPATH/aclImdb_v1.tar.gz -C DATAPATH\n",
        "    \n",
        "    BASE_DIR = 'DATAPATH'\n",
        "    \n",
        "except ModuleNotFoundError:\n",
        "    \n",
        "    if not os.path.exists('Data/glove.6B'):\n",
        "        os.mkdir('/Data/glove.6B')\n",
        "        \n",
        "        url='http://nlp.stanford.edu/data/glove.6B.zip'  \n",
        "        wget.download(url,'Data')  \n",
        "  \n",
        "        temp='Data/glove.6B.zip' \n",
        "        file = ZipFile(temp)  \n",
        "        file.extractall('Data/glove.6B') \n",
        "        file.close()\n",
        "        \n",
        "        \n",
        "        \n",
        "    if not os.path.exists('Data/aclImdb'):\n",
        "        \n",
        "        url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' \n",
        "        wget.download(url,path)\n",
        "        \n",
        "        temp='Data/aclImdb_v1.tar.gz' \n",
        "        tar = tarfile.open(temp, \"r:gz\")\n",
        "        tar.extractall(path)      \n",
        "        tar.close()\n",
        "    \n",
        "    BASE_DIR = 'Data'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvl1qb78fUib"
      },
      "source": [
        "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
        "TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/train')\n",
        "TEST_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/test')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu9xmAZEd7fp"
      },
      "source": [
        "#Within these, I only have a pos/ and a neg/ folder containing text files \n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "#started off from: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "#and from: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmifkoA8b5_N"
      },
      "source": [
        "### Loading and Preprocessing\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI4O1usEb5_O"
      },
      "source": [
        "#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
        "def get_data(data_dir):\n",
        "    texts = []  # list of text samples\n",
        "    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n",
        "    labels = []  # list of label ids\n",
        "    for name in sorted(os.listdir(data_dir)):\n",
        "        path = os.path.join(data_dir, name)\n",
        "        if os.path.isdir(path):\n",
        "            if name=='pos' or name=='neg':\n",
        "                label_id = labels_index[name]\n",
        "                for fname in sorted(os.listdir(path)):\n",
        "                        fpath = os.path.join(path, fname)\n",
        "                        text = open(fpath,encoding='utf8').read()\n",
        "                        texts.append(text)\n",
        "                        labels.append(label_id)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
        "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
        "labels_index = {'pos':1, 'neg':0} \n",
        "\n",
        "#Just to see how the data looks like. \n",
        "#print(train_texts[0])\n",
        "#print(train_labels[0])\n",
        "#print(test_texts[24999])\n",
        "#print(test_labels[24999])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhqM0Jdd7fs",
        "outputId": "87fa1f87-4a45-4921-b04f-3cec24e6a0c3"
      },
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer \n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data. \n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) \n",
        "tokenizer.fit_on_texts(train_texts) \n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes \n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
        "word_index = tokenizer.word_index \n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e0V1-bBb5_d",
        "outputId": "349db20b-de34-450a-8053-be6aef52f790"
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "#This is the data we will use for CNN and RNN training\n",
        "print('Splitting the train data into train and valid is done')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUHqg2vvb5_l",
        "outputId": "295331a8-15a4-45de-9177-a88478639ff2"
      },
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "#print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEastnX8gdxR"
      },
      "source": [
        "### 1D CNN Model with pre-trained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTY-4K-Ob5_t",
        "outputId": "3f795778-06c6-41c5-a3f4-f2273016b020"
      },
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set. \n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Define a 1D CNN model.\n",
            "157/157 [==============================] - 23s 41ms/step - loss: 0.6748 - acc: 0.6068 - val_loss: 0.5651 - val_acc: 0.7040\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.5735 - acc: 0.7004\n",
            "Test accuracy with CNN: 0.7003999948501587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDj2FJzgi_W"
      },
      "source": [
        "### 1D CNN model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI0bISwRb5_w",
        "outputId": "f34c3d27-a2fc-4996-fd11-5027d2bc170a"
      },
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set. \n",
        "cnnmodel.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(x_val, y_val))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "157/157 [==============================] - 14s 82ms/step - loss: 0.5217 - acc: 0.7037 - val_loss: 0.3567 - val_acc: 0.8542\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3553 - acc: 0.8464\n",
            "Test accuracy with CNN: 0.8463600277900696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GwhXpmSgt4H"
      },
      "source": [
        "### LSTM Model with training your own embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBt2Brib5_4",
        "outputId": "08bdb448-ee92-4a7c-d65c-0259730cf340"
      },
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Training the RNN\n",
            "625/625 [==============================] - 1343s 2s/step - loss: 0.4818 - accuracy: 0.7686 - val_loss: 0.3970 - val_accuracy: 0.8374\n",
            "782/782 [==============================] - 219s 280ms/step - loss: 0.3980 - accuracy: 0.8356\n",
            "Test accuracy with RNN: 0.8356000185012817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJYzsZFSg9z-"
      },
      "source": [
        "### LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eymx0IyCb5_-",
        "outputId": "8bdfbaa9-51b9-4fcf-aae8-c3460a7df533"
      },
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel2.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Training the RNN\n",
            "625/625 [==============================] - 1151s 2s/step - loss: 0.6257 - accuracy: 0.6520 - val_loss: 0.5683 - val_accuracy: 0.7186\n",
            "782/782 [==============================] - 218s 279ms/step - loss: 0.5713 - accuracy: 0.7166\n",
            "Test accuracy with RNN: 0.7166399955749512\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}